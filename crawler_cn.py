# -*- coding: utf-8 -*-
import json, requests, os, sys
from bs4 import BeautifulSoup

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

def fetch_futuretools():
    print("ğŸŒ æ•çŒæº 1: FutureTools...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = soup.find_all('a', href=True)
        for l in links:
            if '/tool/' in l['href'] and len(tools) < 10:
                tools.append({"title": "ğŸŒ " + l.get_text(strip=True), "desc": "Silicon Valley Trend", "source": "https://www.futuretools.io" + l['href']})
    except: pass
    return tools

def fetch_topai():
    print("ğŸŒ æ•çŒæº 2: Topai.tools...")
    tools = []
    try:
        res = requests.get("https://topai.tools/new", headers=HEADERS, timeout=15)
        cards = BeautifulSoup(res.text, 'html.parser').select('.card-title a')[:10]
        for c in cards:
            tools.append({"title": "ğŸš€ " + c.get_text(strip=True), "desc": "Global New Release", "source": c['href'] if c['href'].startswith('http') else "https://topai.tools" + c['href']})
    except: pass
    return tools

def fetch_aibot():
    print("ğŸ” æ•çŒæº 3: AIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        cards = BeautifulSoup(res.text, 'html.parser').select('.url-card')[:15]
        for c in cards:
            tools.append({"title": c.select_one('strong').get_text(strip=True), "desc": c.select_one('.url-info p').get_text(strip=True), "source": c.select_one('a')['href']})
    except: pass
    return tools

def main():
    print("ğŸš€ å…¨çƒ AI çŒæ•ç³»ç»Ÿå¯åŠ¨...")
    data = {"cost": [], "efficiency": [], "trend": []}
    
    # æ±‡æ€»æ‰€æœ‰æº
    all_raw = fetch_futuretools() + fetch_topai() + fetch_aibot()
    
    # å¼ºåˆ¶æ ‡è®°ï¼šç”¨äºæ£€æŸ¥åŒæ­¥æ˜¯å¦æˆåŠŸ
    data["efficiency"].append({
        "title": "ğŸš¨ å…¨çƒå¤šæºåŒæ­¥å·²å¼€å¯",
        "desc": "å½“å‰å·²é›†æˆ FutureTools + Topai + å›½å†…ç²¾é€‰",
        "source": "http://cs.bj77.cn"
    })

    # åˆ†ç±»é€»è¾‘
    seen = set()
    for t in all_raw:
        name = t['title'].lower().strip()
        if name not in seen:
            seen.add(name)
            # ç®€å•çš„å…³é”®è¯åˆ†ç±»
            if any(kw in (t['title']+t['desc']).lower() for kw in ['free', 'å…è´¹', 'å¼€æº', 'save']):
                data["cost"].append(t)
            else:
                data["efficiency"].append(t)

    # è¶‹åŠ¿é›·è¾¾ï¼ˆä¿æŒå¤šæºæŠ“å–é€»è¾‘ï¼‰
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        items = BeautifulSoup(res.text, 'html.parser').select('a.article-title')[:6]
        data["trend"] = [{"title": "ğŸ”¥ " + i.get_text(strip=True), "desc": "å®æ—¶çƒ­ç‚¹", "source": "https://36kr.com" + i['href']} for i in items]
    except: pass

    # å†™å…¥æ–‡ä»¶
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç«£å·¥ï¼æ–‡ä»¶å¤§å°: {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e: print(f"âŒ å¤±è´¥: {e}")

if __name__ == "__main__": main()
