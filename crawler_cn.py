# -*- coding: utf-8 -*-
import json, requests, os, sys
from bs4 import BeautifulSoup

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

def fetch_futuretools():
    print("ğŸŒ æ­£åœ¨æ·±åº¦æ‰«æï¼šFutureTools...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        # ä¿®å¤ï¼šä¸å†æ‰¾ divï¼Œç›´æ¥æ‰¾æ‰€æœ‰åŒ…å« /tool/ çš„é“¾æ¥ï¼Œå¹¶å‘ä¸Šå¯»æ‰¾æœ€æ¥è¿‘çš„æ ‡é¢˜æ–‡æœ¬
        all_links = soup.find_all('a', href=True)
        for l in all_links:
            href = l['href']
            if '/tool/' in href and len(tools) < 15:
                # å°è¯•è·å–é“¾æ¥æ–‡æœ¬ï¼Œå¦‚æœå¤ªçŸ­ï¼Œå°è¯•è·å–çˆ¶çº§å…ƒç´ çš„æ–‡æœ¬
                name = l.get_text(strip=True)
                if len(name) < 2: 
                    # å‘ä¸Šæ‰¾ä¸¤å±‚ï¼Œå°è¯•æŠ“å–å¡ç‰‡æ ‡é¢˜
                    parent = l.parent.parent
                    name = parent.get_text(strip=True).split('\n')[0]
                
                if len(name) > 2 and name not in [t['title'] for t in tools]:
                    link = "https://www.futuretools.io" + href if href.startswith('/') else href
                    tools.append({"title": "ğŸŒ " + name[:30], "desc": "Silicon Valley Hot Tool", "source": link})
    except Exception as e: print(f"âš ï¸ FutureTools å¼‚å¸¸: {e}")
    return tools

def fetch_topai():
    print("ğŸŒ æ­£åœ¨æ·±åº¦æ‰«æï¼šTopai...")
    tools = []
    try:
        res = requests.get("https://topai.tools/new", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        # ä¿®å¤ï¼šæ”¾å¼ƒ .card-titleï¼Œå¯»æ‰¾é¡µé¢ä¸­æ‰€æœ‰çš„ h3/h4/h5 é“¾æ¥
        for heading in soup.find_all(['h3', 'h4', 'h5', 'div']):
            link_node = heading.find('a', href=True)
            if link_node and len(tools) < 10:
                name = link_node.get_text(strip=True)
                if name:
                    tools.append({"title": "ğŸš€ " + name[:30], "desc": "International AI Release", "source": link_node['href']})
    except: pass
    return tools

def main():
    print("ğŸš€ AI Hunter å…¨çƒåŒæ­¥ç³»ç»Ÿ [æ·±åº¦ç‰ˆ] å¯åŠ¨...")
    data = {"cost": [], "efficiency": [], "trend": []}
    
    # 1. éªŒè¯æ ‡è®° (å¢åŠ æ—¶é—´æˆ³ï¼Œé˜²æ­¢ç¼“å­˜è¯¯åˆ¤)
    import datetime
    now_str = datetime.datetime.now().strftime("%H:%M:%S")
    data["efficiency"].append({
        "title": f"ğŸš¨ åŒæ­¥æˆåŠŸåé¦ˆ [{now_str}]",
        "desc": "å¦‚æœä½ çœ‹åˆ°æœ¬æ¡ï¼Œè¯´æ˜ GitHub åŒæ­¥é“¾è·¯ 100% æ­£å¸¸",
        "source": "http://cs.bj77.cn"
    })

    # 2. å¤šæºæ··åˆæŠ“å–
    all_raw = fetch_futuretools() + fetch_topai()
    
    # æŠ“å–å›½å†… (AIå·¥å…·é›†)
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        cards = BeautifulSoup(res.text, 'html.parser').select('.url-card')[:15]
        for c in cards:
            all_raw.append({
                "title": c.select_one('strong').get_text(strip=True),
                "desc": c.select_one('.url-info p').get_text(strip=True),
                "source": c.select_one('a')['href']
            })
    except: pass

    # 3. å»é‡ä¸å¼ºåˆ¶åˆ†ç±»
    seen = set()
    for t in all_raw:
        title_clean = t['title'].replace("ğŸŒ ", "").replace("ğŸš€ ", "").lower().strip()
        if title_clean not in seen:
            seen.add(title_clean)
            # åªè¦æ˜¯å¸¦åœ°çƒ/ç«ç®­å›¾æ ‡çš„ï¼Œæˆ–è€…æ˜¯è‹±æ–‡åï¼Œç›´æ¥è¿› efficiency
            if "ğŸŒ" in t['title'] or "ğŸš€" in t['title'] or any(ord(c) < 128 for c in t['title'][:5]):
                data["efficiency"].append(t)
            else:
                data["cost"].append(t)

    # 4. èµ„è®¯é›·è¾¾
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        items = BeautifulSoup(res.text, 'html.parser').select('a.article-title')[:6]
        data["trend"] = [{"title": "ğŸ”¥ " + i.get_text(strip=True), "desc": "å®æ—¶å¿«è®¯", "source": "https://36kr.com" + i['href']} for i in items]
    except: pass

    # 5. å†™å…¥
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç«£å·¥ï¼æ–‡ä»¶å¤§å°: {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e: print(f"âŒ å†™å…¥å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
