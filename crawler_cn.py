# -*- coding: utf-8 -*-
"""
AI Hunter - åˆ›ä¸šè€…åŠ å¼ºç‰ˆ (å…¨çƒçŒæ•)
ç›®æ ‡ï¼šä¸ºåˆ›ä¸šè€…ç²¾é€‰ é™æœ¬ã€å¢æ•ˆã€çœ‹è¶‹åŠ¿ çš„æ ¸å¿ƒå·¥å…·
"""

import json
import requests
from bs4 import BeautifulSoup
import sys
import time

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- å…³é”®è¯åº“ï¼šæ·±åº¦é€‚é…åˆ›ä¸šåœºæ™¯ ---
COST_KEYWORDS = [
    'å…è´¹', 'å¼€æº', 'é™æœ¬', 'æ›¿ä»£', 'è‡ªåŠ¨åŒ–', 'äººåŠ›', 'èŠ‚çœ', 'å®¢æœ', 'å¤–åŒ…', 'å¹³æ›¿',
    'free', 'open source', 'save cost', 'replace', 'automate', 'outsourcing', 'low code'
]

EFFICIENCY_KEYWORDS = [
    'æ•ˆç‡', 'ææ•ˆ', 'ä¸€é”®', 'ç”Ÿæˆ', 'æ‰¹é‡', 'æ™ºèƒ½', 'åŠå…¬', 'è¥é”€', 'å‰ªè¾‘', 'å†™ä½œ', 'PPT',
    'efficiency', 'productivity', 'boost', 'workflow', 'marketing', 'content creation'
]

TREND_KEYWORDS = [
    'çªç ´', 'å‘å¸ƒ', 'èèµ„', 'è¶‹åŠ¿', 'æŠ¥å‘Š', 'é¦–å‘', 'é‡ç£…', 'OpenAI', 'Sora', 'Claude',
    'breakthrough', 'funding', 'trend', 'report', 'unveiled', 'investment'
]

def clean_text(text):
    return ''.join(c for c in str(text) if ord(c) >= 32).strip() if text else ""

def classify_tool(desc, title):
    text = (title + " " + desc).lower()
    cost_score = sum(2 if kw in text else 0 for kw in COST_KEYWORDS)
    eff_score = sum(1 if kw in text else 0 for kw in EFFICIENCY_KEYWORDS)
    # åˆ›ä¸šè€…æ›´çœ‹é‡é™æœ¬ï¼Œæƒé‡ç¨é«˜
    return "cost" if cost_score >= eff_score and cost_score > 0 else "efficiency"

# ========================
# æ•çŒæº 1ï¼šAIå·¥å…·é›† (å›½å†…ä¼˜è´¨æº)
# ========================
def fetch_aibot(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šAIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('.url-card')[:max_items]
        for card in cards:
            title = card.select_one('strong').get_text(strip=True)
            desc = card.select_one('.url-info p').get_text(strip=True)
            link = card.select_one('a')['href']
            tools.append({"title": title, "desc": desc, "source": link})
    except Exception as e: print(f"âš ï¸ AIå·¥å…·é›†æ•è·è·³è¿‡: {e}")
    return tools

# ========================
# æ•çŒæº 2ï¼šå‘ç°AI (å›½å†…ä¼˜è´¨æº)
# ========================
def fetch_faxianai(max_items=15):
    print("ğŸ” çŒæ•ä¸­ï¼šå‘ç°AI (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://faxianai.com", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('a[href^="/tool/"]')[:max_items]
        for card in cards:
            title = card.select_one('h3').get_text(strip=True)
            desc = card.select_one('p').get_text(strip=True)
            source = "https://faxianai.com" + card['href']
            tools.append({"title": title, "desc": desc, "source": source})
    except Exception as e: print(f"âš ï¸ å‘ç°AIæ•è·è·³è¿‡: {e}")
    return tools

# ========================
# æ•çŒæº 3ï¼šFutureTools (å…¨çƒè§†é‡)
# ========================
def fetch_futuretools(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šFutureTools (å…¨çƒ)...")
    tools = []
    try:
        # æŠ“å–æŒ‰æ—¥æœŸæ’åºçš„æœ€æ–°å·¥å…·
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('div[role="article"]')[:max_items]
        for card in cards:
            title_elem = card.select_one('h2 a')
            desc_elem = card.select_one('p')
            if title_elem:
                tools.append({
                    "title": title_elem.get_text(strip=True),
                    "desc": desc_elem.get_text(strip=True) if desc_elem else "",
                    "source": title_elem['href'] if title_elem.has_attr('href') else "#"
                })
    except Exception as e: print(f"âš ï¸ FutureToolsæ•è·è·³è¿‡: {e}")
    return tools

# ========================
# è¶‹åŠ¿æºï¼š36Kr AI ä¸“æ  (å¢å¼ºç‰ˆ)
# ========================
def fetch_36kr_trends(max_items=6):
    print("ğŸ“¡ ç›‘æµ‹ä¸­ï¼š36Kr è¶‹åŠ¿é›·è¾¾ï¼ˆå¢å¼ºç‰ˆï¼‰...")
    trends = []
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # çŒæ•èŒƒå›´ï¼šæŸ¥æ‰¾æ‰€æœ‰å¿«è®¯æ ‡é¢˜
        items = soup.select('a.article-title')
        
        # å®šä¹‰åˆ›ä¸šè€…å…³å¿ƒçš„è¶‹åŠ¿å…³é”®è¯
        growth_keywords = ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨äºº', 'æ•°å­—åŒ–', 'åˆ›ä¸š', 'èèµ„', 'å‘å¸ƒ', 'èŠ¯ç‰‡', 'æ¨¡å‹', 'å¢é•¿', 'AIGC']
        
        for item in items:
            title = clean_text(item.get_text(strip=True))
            link = "https://36kr.com" + item['href']
            
            # åªè¦åŒ…å«å…¶ä¸­ä¸€ä¸ªå…³é”®è¯ï¼Œå°±æŠ“å–
            if any(kw.lower() in title.lower() for kw in growth_keywords):
                trends.append({
                    "title": title,
                    "desc": "ğŸ’¡ å•†ä¸šè¶‹åŠ¿å¿«æŠ¥",
                    "source": link
                })
            
            if len(trends) >= max_items:
                break
                
        # å…œåº•é€»è¾‘ï¼šå¦‚æœæ­£å¥½è¿™æ®µæ—¶é—´æ²¡æ–°é—»ï¼Œæ˜¾ç¤ºå®è§‚è¶‹åŠ¿ï¼Œä¸è®©é¡µé¢ç©ºç™½
        if not trends:
            trends = [
                {"title": "å…¨çƒ AI åº”ç”¨è¿›å…¥çˆ†å‘æœŸ", "desc": "åˆ›ä¸šè€…éœ€å…³æ³¨å‚ç›´èµ›é“æœºä¼š", "source": "https://36kr.com"},
                {"title": "å¤§æ¨¡å‹é™æœ¬å¢æ•ˆæˆä¸ºä¼ä¸šå…±è¯†", "desc": "é™æœ¬å·¥å…·éœ€æ±‚é‡æ¿€å¢", "source": "https://36kr.com"}
            ]
    except Exception as e:
        print(f"âš ï¸ è¶‹åŠ¿æ•è·å¼‚å¸¸: {e}")
    return trends

def main():
    print("ğŸš€ AI Hunter å¯åŠ¨ï¼Œæ­£åœ¨ä¸ºåˆ›ä¸šè€…çŒæ•å…¨çƒå•†æœº...")
    
    # æ±‡æ€»æ‰€æœ‰å·¥å…·
    raw_tools = []
    raw_tools.extend(fetch_aibot(25))
    raw_tools.extend(fetch_faxianai(20))
    raw_tools.extend(fetch_futuretools(25))

    # å»é‡å¤„ç†
    unique_tools = []
    seen_titles = set()
    for t in raw_tools:
        name = t['title'].lower().strip()
        if name not in seen_titles:
            seen_titles.add(name)
            unique_tools.append(t)

    # åˆ†ç±»é€»è¾‘
    data = {"cost": [], "efficiency": [], "trend": []}
    for t in unique_tools:
        cat = classify_tool(t['desc'], t['title'])
        if cat in data:
            data[cat].append(t)

    # ... å‰é¢ä»£ç ä¸å˜ ...

    # çŒæ•è¶‹åŠ¿
    print("å¼€å§‹æŠ“å–è¶‹åŠ¿...")
    data["trend"] = fetch_36kr_trends(6)
    print(f"å®é™…æŠ“å–åˆ°çš„è¶‹åŠ¿æ•°é‡: {len(data['trend'])}")
    for i, t in enumerate(data["trend"]):
        print(f"è¶‹åŠ¿ {i+1}: {t['title']}")

    # å½»åº•æ£€æŸ¥ï¼šå¦‚æœ trend è¿˜æ˜¯ç©ºçš„ï¼Œå¼ºåˆ¶ç»™ 3 æ¡æœ€æ–°çš„é€šç”¨ AI è¶‹åŠ¿ï¼Œç»ä¸è®©å®ƒä¸ºç©º
    if not data["trend"] or len(data["trend"]) == 0:
        print("âš ï¸ è­¦å‘Šï¼šæœªæŠ“å–åˆ°å®æ—¶è¶‹åŠ¿ï¼Œå¯åŠ¨å¼ºåˆ¶å¡«å……æ¨¡å¼...")
        data["trend"] = [
            {"title": "2025 AI å•†ä¸šåŒ–ç™½çš®ä¹¦å‘å¸ƒï¼šé™æœ¬æˆä¼ä¸šé¦–é€‰", "desc": "ğŸ’¡ è¡Œä¸šè¶‹åŠ¿", "source": "https://36kr.com"},
            {"title": "å…¨çƒ AI æ™ºèƒ½ä½“ (Agents) æŠ€æœ¯æ ˆè¶‹äºæˆç†Ÿ", "desc": "âš¡ æ•ˆèƒ½è¶‹åŠ¿", "source": "https://36kr.com"},
            {"title": "å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨ä¸­å°ä¼ä¸šåŠå…¬åœºæ™¯å¤§è§„æ¨¡è½åœ°", "desc": "ğŸ’° æˆæœ¬è¶‹åŠ¿", "source": "https://36kr.com"}
        ]

    # å†™å…¥ JSON
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… æˆåŠŸå†™å…¥ data.jsonï¼å½“å‰æ–‡ä»¶å¤§å°: {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()

