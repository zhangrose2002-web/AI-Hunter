# -*- coding: utf-8 -*-
"""
AI Hunter - åˆ›ä¸šè€…åŠ å¼ºç‰ˆ (å…¨çƒçŒæ• + è‡ªåŠ¨é“¾æ¥ä½“æ£€)
åŠŸèƒ½ï¼šæŠ“å–æ•°æ® -> è‡ªåŠ¨æ£€æµ‹é“¾æ¥ -> å‰”é™¤å¼‚å¸¸(403/404/è¶…æ—¶) -> ç”Ÿæˆ data.json
"""

import json
import requests
from bs4 import BeautifulSoup
import sys
import time
import os

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- å…³é”®è¯åº“ ---
COST_KEYWORDS = ['å…è´¹', 'å¼€æº', 'é™æœ¬', 'æ›¿ä»£', 'è‡ªåŠ¨åŒ–', 'äººåŠ›', 'èŠ‚çœ', 'å¹³æ›¿', 'free', 'save cost', 'low code']
EFFICIENCY_KEYWORDS = ['æ•ˆç‡', 'ææ•ˆ', 'ä¸€é”®', 'ç”Ÿæˆ', 'æ‰¹é‡', 'æ™ºèƒ½', 'åŠå…¬', 'è¥é”€', 'å†™ä½œ', 'efficiency', 'productivity']

def is_link_valid(url):
    """
    ã€æ ¸å¿ƒæ–°å¢ã€‘é“¾æ¥ä½“æ£€å‡½æ•°
    å°è¯•è®¿é—®é“¾æ¥ï¼Œå¦‚æœè¿”å›é200çŠ¶æ€ç æˆ–è¶…æ—¶ï¼Œåˆ™è§†ä¸ºå¼‚å¸¸
    """
    if not url or url == "#":
        return False
    try:
        # ä½¿ç”¨ HEAD è¯·æ±‚å¿«é€Ÿæ£€æµ‹ï¼Œè®¾ç½® 5 ç§’è¶…æ—¶
        # allow_redirects=True å…è®¸è‡ªåŠ¨è·³è½¬åˆ°æœ€ç»ˆåœ°å€
        response = requests.head(url, headers=HEADERS, timeout=5, allow_redirects=True)
        
        # å¦‚æœ HEAD è¯·æ±‚ä¸è¢«å…è®¸(æœ‰äº›ç«™æŠ¥405)ï¼Œåˆ™å°è¯• GET è¯·æ±‚åªè¯»å–å‰ 1 å­—èŠ‚
        if response.status_code >= 400:
            response = requests.get(url, headers=HEADERS, timeout=5, stream=True)
            
        if response.status_code == 200:
            return True
        else:
            print(f"âŒ é“¾æ¥å¤±æ•ˆ ({response.status_code}): {url}")
            return False
    except Exception as e:
        print(f"âŒ é“¾æ¥æ— æ³•è¿æ¥: {url} | é”™è¯¯: {e}")
        return False

def clean_text(text):
    return ''.join(c for c in str(text) if ord(c) >= 32).strip() if text else ""

def classify_tool(desc, title):
    text = (title + " " + desc).lower()
    cost_score = sum(2 if kw in text else 0 for kw in COST_KEYWORDS)
    eff_score = sum(1 if kw in text else 0 for kw in EFFICIENCY_KEYWORDS)
    return "cost" if cost_score >= eff_score and cost_score > 0 else "efficiency"

# --- æ•çŒå‡½æ•° (é€»è¾‘åŒåŸç‰ˆ) ---
def fetch_aibot(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šAIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('.url-card')[:max_items]
        for card in cards:
            title = card.select_one('strong').get_text(strip=True)
            desc = card.select_one('.url-info p').get_text(strip=True)
            link = card.select_one('a')['href']
            # åœ¨è¿™é‡Œç›´æ¥è¿›è¡Œä½“æ£€
            if is_link_valid(link):
                tools.append({"title": title, "desc": desc, "source": link})
    except Exception as e: print(f"âš ï¸ AIå·¥å…·é›†æ•è·è·³è¿‡: {e}")
    return tools

def fetch_faxianai(max_items=15):
    print("ğŸ” çŒæ•ä¸­ï¼šå‘ç°AI (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://faxianai.com", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('a[href^="/tool/"]')[:max_items]
        for card in cards:
            title = card.select_one('h3').get_text(strip=True)
            desc = card.select_one('p').get_text(strip=True)
            source = "https://faxianai.com" + card['href']
            # æ³¨æ„ï¼šæºç«™é“¾æ¥ä¹Ÿè¦æ£€æŸ¥
            if is_link_valid(source):
                tools.append({"title": title, "desc": desc, "source": source})
    except Exception as e: print(f"âš ï¸ å‘ç°AIæ•è·è·³è¿‡: {e}")
    return tools

def fetch_futuretools(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šFutureTools (å…¨çƒ)...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('div[role="article"]')[:max_items]
        for card in cards:
            title_elem = card.select_one('h2 a')
            desc_elem = card.select_one('p')
            if title_elem:
                link = title_elem['href'] if title_elem.has_attr('href') else "#"
                if is_link_valid(link):
                    tools.append({
                        "title": title_elem.get_text(strip=True),
                        "desc": desc_elem.get_text(strip=True) if desc_elem else "",
                        "source": link
                    })
    except Exception as e: print(f"âš ï¸ FutureToolsæ•è·è·³è¿‡: {e}")
    return tools

def fetch_36kr_trends(max_items=6):
    print("ğŸ“¡ ç›‘æµ‹ä¸­ï¼š36Kr è¶‹åŠ¿é›·è¾¾...")
    trends = []
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('a.article-title')
        growth_keywords = ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨äºº', 'æ•°å­—åŒ–', 'åˆ›ä¸š', 'èèµ„', 'AIGC']
        
        for item in items:
            title = clean_text(item.get_text(strip=True))
            link = "https://36kr.com" + item['href']
            if any(kw.lower() in title.lower() for kw in growth_keywords):
                # è¶‹åŠ¿é“¾æ¥ä¹Ÿè¦æ£€æŸ¥
                if is_link_valid(link):
                    trends.append({"title": title, "desc": "ğŸ’¡ å•†ä¸šè¶‹åŠ¿å¿«æŠ¥", "source": link})
            if len(trends) >= max_items: break
    except Exception as e: print(f"âš ï¸ è¶‹åŠ¿æ•è·å¼‚å¸¸: {e}")
    return trends

def main():
    print("ğŸš€ AI Hunter å¯åŠ¨ï¼Œæ­£åœ¨è¿›è¡Œâ€˜è´¨é‡ä¿éšœå‹â€™çŒæ•...")
    
    raw_tools = []
    raw_tools.extend(fetch_aibot(20))
    raw_tools.extend(fetch_faxianai(15))
    raw_tools.extend(fetch_futuretools(20))

    # å»é‡å¤„ç†
    unique_tools = []
    seen_titles = set()
    for t in raw_tools:
        name = t['title'].lower().strip()
        if name not in seen_titles:
            seen_titles.add(name)
            unique_tools.append(t)

    data = {"cost": [], "efficiency": [], "trend": []}
    for t in unique_tools:
        cat = classify_tool(t['desc'], t['title'])
        if cat in data:
            data[cat].append(t)

    data["trend"] = fetch_36kr_trends(6)

    # å…œåº•è¶‹åŠ¿
    if not data["trend"]:
        data["trend"] = [
            {"title": "2025 AI å•†ä¸šåŒ–è¶‹åŠ¿ï¼šé™æœ¬å¢æ•ˆæˆæ ¸å¿ƒ", "desc": "ğŸ’¡ è¡Œä¸šè¶‹åŠ¿", "source": "https://36kr.com"},
            {"title": "å…¨çƒ AI Agents æŠ€æœ¯æ ˆè¶‹äºæˆç†Ÿ", "desc": "âš¡ æ•ˆèƒ½è¶‹åŠ¿", "source": "https://36kr.com"}
        ]

    # å†™å…¥ JSON
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ data.jsonï¼å¼‚å¸¸é“¾æ¥å·²è‡ªåŠ¨è¿‡æ»¤ã€‚")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
