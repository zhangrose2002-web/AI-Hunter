# -*- coding: utf-8 -*-
"""
AI Hunter - åˆ›ä¸šè€…è´¨é‡ä¿éšœç‰ˆ
1. ä¿®å¤ os æ¨¡å—ç¼ºå¤±æŠ¥é”™
2. å¢å¼ºä¸­è‹±æ–‡åˆ†ç±»è¯†åˆ«ï¼Œæ”¯æŒå…¨çƒæº
3. è‡ªåŠ¨å‰”é™¤ 403/404/å¤±æ•ˆé“¾æ¥
"""

import json
import requests
from bs4 import BeautifulSoup
import sys
import time
import os  # [æ ¸å¿ƒä¿®å¤] è¡¥å…¨ç¼ºå¤±çš„ os æ¨¡å—ï¼Œè§£å†³å†™å…¥å¤±è´¥é—®é¢˜

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- å…³é”®è¯åº“ï¼šæ–°å¢è‹±æ–‡æ”¯æŒï¼Œç¡®ä¿ FutureTools çš„æ´‹å·¥å…·èƒ½è¢«æ­£ç¡®åˆ†ç±» ---
COST_KEYWORDS = [
    'å…è´¹', 'å¼€æº', 'é™æœ¬', 'æ›¿ä»£', 'èŠ‚çœ', 'å¹³æ›¿',
    'free', 'open source', 'save cost', 'replace', 'automate', 'low code'
]

EFFICIENCY_KEYWORDS = [
    'æ•ˆç‡', 'ææ•ˆ', 'ä¸€é”®', 'ç”Ÿæˆ', 'æ‰¹é‡', 'æ™ºèƒ½', 'åŠå…¬', 'å‰ªè¾‘', 'å†™ä½œ',
    'efficiency', 'productivity', 'boost', 'workflow', 'marketing', 'content creation'
]

def is_link_valid(url):
    """ è‡ªåŠ¨æ£€æµ‹é“¾æ¥æ˜¯å¦å¯ç”¨ï¼Œè·³è¿‡ trae.cn ç­‰ 403 é”™è¯¯ """
    if not url or url == "#":
        return False
    try:
        # å°è¯•å¿«é€Ÿæ£€æµ‹
        res = requests.head(url, headers=HEADERS, timeout=5, allow_redirects=True)
        if res.status_code >= 400:
            res = requests.get(url, headers=HEADERS, timeout=5, stream=True)
        return res.status_code == 200
    except:
        return False

def clean_text(text):
    return ''.join(c for c in str(text) if ord(c) >= 32).strip() if text else ""

def classify_tool(desc, title):
    """ åˆ†ç±»é€»è¾‘ï¼šé€šè¿‡ä¸­è‹±æ–‡å…³é”®è¯è®¡ç®—å¾—åˆ† """
    text = (title + " " + desc).lower()
    cost_score = sum(2 if kw in text else 0 for kw in COST_KEYWORDS)
    eff_score = sum(1 if kw in text else 0 for kw in EFFICIENCY_KEYWORDS)
    
    if cost_score == 0 and eff_score == 0:
        return "efficiency"  # å…œåº•ï¼šæœªåŒ¹é…åˆ°å…³é”®è¯çš„å›½å¤–å·¥å…·é»˜è®¤å…¥ææ•ˆç±»ï¼Œé˜²æ­¢ä¸¢å¤±
    return "cost" if cost_score >= eff_score else "efficiency"

# --- æ•çŒå‡½æ•°ç¾¤ ---
def fetch_aibot(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šAIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('.url-card')[:max_items]
        for card in cards:
            title = card.select_one('strong').get_text(strip=True)
            desc = card.select_one('.url-info p').get_text(strip=True)
            link = card.select_one('a')['href']
            if is_link_valid(link): tools.append({"title": title, "desc": desc, "source": link})
    except Exception as e: print(f"âš ï¸ AIå·¥å…·é›†è·³è¿‡: {e}")
    return tools

def fetch_faxianai(max_items=15):
    print("ğŸ” çŒæ•ä¸­ï¼šå‘ç°AI (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://faxianai.com", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('a[href^="/tool/"]')[:max_items]
        for card in cards:
            title = card.select_one('h3').get_text(strip=True)
            desc = card.select_one('p').get_text(strip=True)
            source = "https://faxianai.com" + card['href']
            if is_link_valid(source): tools.append({"title": title, "desc": desc, "source": source})
    except Exception as e: print(f"âš ï¸ å‘ç°AIè·³è¿‡: {e}")
    return tools

def fetch_futuretools(max_items=25):
    print("ğŸ” çŒæ•ä¸­ï¼šFutureTools (å…¨çƒæº)...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('div[role="article"]')[:max_items]
        for card in cards:
            title_elem = card.select_one('h2 a')
            desc_elem = card.select_one('p')
            if title_elem:
                link = title_elem['href'] if title_elem.has_attr('href') else "#"
                if is_link_valid(link):
                    tools.append({
                        "title": title_elem.get_text(strip=True),
                        "desc": desc_elem.get_text(strip=True) if desc_elem else "",
                        "source": link
                    })
    except Exception as e: print(f"âš ï¸ FutureToolsè·³è¿‡: {e}")
    return tools

def fetch_36kr_trends(max_items=6):
    print("ğŸ“¡ ç›‘æµ‹ä¸­ï¼š36Kr è¶‹åŠ¿é›·è¾¾...")
    trends = []
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('a.article-title')
        growth_keywords = ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨äºº', 'åˆ›ä¸š', 'èèµ„', 'AIGC', 'èŠ¯ç‰‡']
        for item in items:
            title = clean_text(item.get_text(strip=True))
            link = "https://36kr.com" + item['href']
            if any(kw in title for kw in growth_keywords) and is_link_valid(link):
                trends.append({"title": title, "desc": "ğŸ’¡ å•†ä¸šè¶‹åŠ¿å¿«æŠ¥", "source": link})
            if len(trends) >= max_items: break
    except Exception as e: print(f"âš ï¸ è¶‹åŠ¿æŠ“å–å¼‚å¸¸: {e}")
    return trends

def main():
    print("ğŸš€ AI Hunter è´¨é‡ä¿éšœç‰ˆå¯åŠ¨...")
    
    # æ±‡æ€»
    raw_tools = []
    raw_tools.extend(fetch_aibot(20))
    raw_tools.extend(fetch_faxianai(15))
    raw_tools.extend(fetch_futuretools(25))

    # å»é‡
    unique_tools = []
    seen_titles = set()
    for t in raw_tools:
        name = t['title'].lower().strip()
        if name not in seen_titles:
            seen_titles.add(name)
            unique_tools.append(t)

    # åˆ†ç±»
    data = {"cost": [], "efficiency": [], "trend": []}
    for t in unique_tools:
        cat = classify_tool(t['desc'], t['title'])
        data[cat].append(t)

    # è¶‹åŠ¿
    data["trend"] = fetch_36kr_trends(6)
    if not data["trend"]:
        data["trend"] = [{"title": "å…¨çƒAIå•†ä¸šåŒ–ç™½çš®ä¹¦ï¼šé™æœ¬æˆæ ¸å¿ƒ", "desc": "ğŸ’¡ è¡Œä¸šè¶‹åŠ¿", "source": "https://36kr.com"}]

    # å†™å…¥ JSON
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… å†™å…¥æˆåŠŸï¼å½“å‰æ–‡ä»¶å¤§å°: {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
