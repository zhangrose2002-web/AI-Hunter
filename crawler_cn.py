# -*- coding: utf-8 -*-
"""
AI Hunter - å…¨çƒå•†æœºçŒæ•ç‰ˆ
æ ¸å¿ƒä¿®æ­£ï¼š
1. è¡¥å…¨ os æ¨¡å—ï¼Œç¡®ä¿ data.json å†™å…¥æˆåŠŸ
2. å…¼å®¹ä¸­è‹±æ–‡å…³é”®è¯ï¼Œç¡®ä¿å›½å¤–å·¥å…·ä¸æ‰é˜Ÿ
3. è‡ªåŠ¨å‰”é™¤ 403/404 å¤±æ•ˆé“¾æ¥
"""

import json
import requests
from bs4 import BeautifulSoup
import sys
import time
import os  # [æ ¸å¿ƒä¿®å¤] å¿…é¡»å¯¼å…¥ï¼Œå¦åˆ™å†™å…¥æ–‡ä»¶ä¼šæŠ¥é”™

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- å…³é”®è¯åº“ï¼šå¢åŠ è‹±æ–‡ï¼Œè®©æœºå™¨äººè®¤è¯†å›½å¤–å·¥å…· ---
COST_KEYWORDS = ['å…è´¹', 'å¼€æº', 'é™æœ¬', 'èŠ‚çœ', 'å¹³æ›¿', 'free', 'open source', 'save cost', 'low code']
EFFICIENCY_KEYWORDS = ['ææ•ˆ', 'æ™ºèƒ½', 'ä¸€é”®', 'åŠå…¬', 'å‰ªè¾‘', 'å†™ä½œ', 'efficiency', 'productivity', 'boost', 'automate']

def is_link_valid(url):
    """ é“¾æ¥ä½“æ£€ï¼šè‡ªåŠ¨è·³è¿‡ 403 ç­‰æ‰“ä¸å¼€çš„ç½‘ç«™ """
    if not url or url == "#": return False
    try:
        res = requests.head(url, headers=HEADERS, timeout=5, allow_redirects=True)
        if res.status_code >= 400:
            res = requests.get(url, headers=HEADERS, timeout=5, stream=True)
        return res.status_code == 200
    except:
        return False

def clean_text(text):
    return ''.join(c for c in str(text) if ord(c) >= 32).strip() if text else ""

def classify_tool(desc, title):
    """ åˆ†ç±»é€»è¾‘ï¼šç¡®ä¿æ‰€æœ‰æŠ“åˆ°çš„å·¥å…·éƒ½èƒ½è¢«å½’ç±»ï¼Œä¸æ¶ˆå¤± """
    text = (title + " " + desc).lower()
    cost_score = sum(2 if kw in text else 0 for kw in COST_KEYWORDS)
    eff_score = sum(1 if kw in text else 0 for kw in EFFICIENCY_KEYWORDS)
    # åªè¦æ˜¯æŠ“åˆ°çš„å·¥å…·ï¼Œé»˜è®¤è‡³å°‘åˆ†å…¥æ•ˆç‡ç±»ï¼Œä¸è®©å®ƒåœ¨ JSON ä¸­æ¶ˆå¤±
    return "cost" if cost_score >= eff_score and cost_score > 0 else "efficiency"

# --- æŠ“å–å‡½æ•° ---
def fetch_aibot(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šAIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('.url-card')[:max_items]
        for card in cards:
            title = card.select_one('strong').get_text(strip=True)
            desc = card.select_one('.url-info p').get_text(strip=True)
            link = card.select_one('a')['href']
            if is_link_valid(link): tools.append({"title": title, "desc": desc, "source": link})
    except Exception as e: print(f"âš ï¸ AIå·¥å…·é›†è·³è¿‡: {e}")
    return tools

def fetch_futuretools(max_items=25):
    print("ğŸ” çŒæ•ä¸­ï¼šFutureTools (å…¨çƒæº)...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('div[role="article"]')[:max_items]
        for card in cards:
            title_elem = card.select_one('h2 a')
            desc_elem = card.select_one('p')
            if title_elem:
                link = title_elem['href']
                if is_link_valid(link):
                    tools.append({
                        "title": title_elem.get_text(strip=True),
                        "desc": desc_elem.get_text(strip=True) if desc_elem else "",
                        "source": link
                    })
    except Exception as e: print(f"âš ï¸ FutureToolsè·³è¿‡: {e}")
    return tools

def fetch_36kr_trends(max_items=6):
    print("ğŸ“¡ ç›‘æµ‹ä¸­ï¼š36Kr è¶‹åŠ¿é›·è¾¾...")
    trends = []
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('a.article-title')
        for item in items:
            title = clean_text(item.get_text(strip=True))
            link = "https://36kr.com" + item['href']
            if is_link_valid(link):
                trends.append({"title": title, "desc": "ğŸ’¡ å•†ä¸šè¶‹åŠ¿å¿«æŠ¥", "source": link})
            if len(trends) >= max_items: break
    except Exception as e: print(f"âš ï¸ è¶‹åŠ¿æŠ“å–å¼‚å¸¸: {e}")
    return trends

def main():
    print("ğŸš€ AI Hunter å¯åŠ¨...")
    raw_tools = []
    raw_tools.extend(fetch_aibot(20))
    raw_tools.extend(fetch_futuretools(25)) # æŠ“å–å›½å¤–æº

    # å»é‡
    unique_tools = []
    seen_titles = set()
    for t in raw_tools:
        name = t['title'].lower().strip()
        if name not in seen_titles:
            seen_titles.add(name)
            unique_tools.append(t)

    # åˆ†ç±»
    data = {"cost": [], "efficiency": [], "trend": []}
    for t in unique_tools:
        cat = classify_tool(t['desc'], t['title'])
        data[cat].append(t)

    data["trend"] = fetch_36kr_trends(6)

    # å†™å…¥ JSON
    try:
        # ä½¿ç”¨ os.path.abspath ç¡®ä¿è·¯å¾„æ­£ç¡®
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        # è¿™é‡Œä¼šç”¨åˆ° os æ¨¡å—
        print(f"\nâœ… å†™å…¥æˆåŠŸï¼å½“å‰æ–‡ä»¶å¤§å°: {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
