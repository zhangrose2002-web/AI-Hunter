# -*- coding: utf-8 -*-
import json, requests, os, sys
from bs4 import BeautifulSoup

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

def is_link_valid(url):
    """ é“¾æ¥ä½“æ£€ï¼šä»…é’ˆå¯¹å›½å†…æºï¼Œé˜²æ­¢ 403 å¯¼è‡´è¿‡æ»¤ """
    if not url or url == "#": return False
    try:
        # å›½å¤–æºå®¹æ˜“è¯¯æŠ¥ 403ï¼Œå› æ­¤ä½“æ£€é€»è¾‘åªç•™ç»™å›½å†…
        res = requests.get(url, headers=HEADERS, timeout=5, stream=True)
        return res.status_code < 400
    except: return False

def fetch_futuretools():
    print("ğŸŒ æ­£åœ¨çŒæ•ï¼šFutureTools (å…¨çƒæº)...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        # æ”¹ç”¨æ›´å¼ºå¤§çš„é“¾æ¥åŒ¹é…ï¼Œä¸ä¾èµ–ç‰¹å®šçš„ div ç»“æ„
        links = soup.find_all('a', href=True)
        for l in links:
            href = l['href']
            if '/tool/' in href and len(tools) < 15:
                title = l.get_text(strip=True)
                if len(title) > 3:
                    tools.append({
                        "title": "ğŸŒ " + title,
                        "desc": "Global Emerging AI Technology",
                        "source": "https://www.futuretools.io" + href if href.startswith('/') else href
                    })
    except: pass
    return tools

def fetch_topai():
    print("ğŸŒ æ­£åœ¨çŒæ•ï¼šTopai.tools (å¤‡ç”¨æº)...")
    tools = []
    try:
        res = requests.get("https://topai.tools/new", headers=HEADERS, timeout=15)
        cards = BeautifulSoup(res.text, 'html.parser').select('.card-title a')[:10]
        for c in cards:
            tools.append({"title": "ğŸš€ " + c.get_text(strip=True), "desc": "International AI Release", "source": c['href']})
    except: pass
    return tools

def fetch_aibot():
    print("ğŸ” çŒæ•ä¸­ï¼šAIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        cards = BeautifulSoup(res.text, 'html.parser').select('.url-card')[:15]
        for c in cards:
            link = c.select_one('a')['href']
            if is_link_valid(link): # å›½å†…é“¾æ¥ä¾ç„¶ä¿æŒä½“æ£€
                tools.append({
                    "title": c.select_one('strong').get_text(strip=True),
                    "desc": c.select_one('.url-info p').get_text(strip=True),
                    "source": link
                })
    except: pass
    return tools

def main():
    print("ğŸš€ AI Hunter å…¨çƒåŒæ­¥ç³»ç»Ÿå¯åŠ¨...")
    data = {"cost": [], "efficiency": [], "trend": []}
    
    # å¼ºåˆ¶æ ‡è®°ï¼šçœ‹åˆ°è¿™ä¸ªè¯´æ˜åŒæ­¥æˆåŠŸäº†ï¼
    data["efficiency"].append({
        "title": "ğŸš¨ ç³»ç»Ÿæ›´æ–°ï¼šå…¨çƒå¤šæºåŒæ­¥å·²å¼€å¯ (2025)",
        "desc": "å¦‚æœçœ‹åˆ°æ­¤æ¡ï¼Œè¯´æ˜æ•°æ®å·²æˆåŠŸåŒæ­¥è‡³æœåŠ¡å™¨",
        "source": "http://cs.bj77.cn"
    })

    # æŠ“å–ä¸‰æ–¹å·¥å…·å¹¶æ±‡æ€»
    all_tools = fetch_futuretools() + fetch_topai() + fetch_aibot()
    
    # åˆ†ç±»é€»è¾‘ï¼šæ”¾å®½åˆ†ç±»åˆ†å€¼ï¼Œä¸æ¼æ‰ä¸€ä¸ª
    seen = set()
    for t in all_tools:
        name = t['title'].lower().strip()
        if name not in seen:
            seen.add(name)
            # åªè¦å¸¦æœ‰ ğŸŒ æˆ– ğŸš€ å›¾æ ‡çš„ï¼Œé€šé€šåˆ†å…¥ efficiency
            if any(icon in t['title'] for icon in ["ğŸŒ", "ğŸš€"]):
                data["efficiency"].append(t)
            elif any(kw in (t['title'] + t['desc']).lower() for kw in ['å…è´¹', 'å¼€æº', 'free', 'save']):
                data["cost"].append(t)
            else:
                data["efficiency"].append(t)

    # èµ„è®¯é›·è¾¾å¤šæºæŠ“å–
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        items = BeautifulSoup(res.text, 'html.parser').select('a.article-title')[:6]
        data["trend"] = [{"title": "ğŸ”¥ " + i.get_text(strip=True), "desc": "å®æ—¶çƒ­ç‚¹", "source": "https://36kr.com" + i['href']} for i in items]
    except: pass

    # å†™å…¥æ–‡ä»¶
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç«£å·¥ï¼æ–‡ä»¶å¤§å°: {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e: print(f"âŒ å†™å…¥å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
