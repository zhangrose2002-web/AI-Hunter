# -*- coding: utf-8 -*-
import json
import requests
from bs4 import BeautifulSoup
import sys
import time
import os

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- å…³é”®è¯åº“ï¼šæ”¯æŒä¸­è‹±æ–‡è¯†åˆ« ---
COST_KEYWORDS = ['å…è´¹', 'å¼€æº', 'é™æœ¬', 'èŠ‚çœ', 'å¹³æ›¿', 'free', 'open source', 'save cost', 'low code']
EFFICIENCY_KEYWORDS = ['ææ•ˆ', 'æ™ºèƒ½', 'ä¸€é”®', 'åŠå…¬', 'å‰ªè¾‘', 'å†™ä½œ', 'efficiency', 'productivity', 'boost', 'automate']

def is_link_valid(url):
    """ é“¾æ¥ä½“æ£€ï¼šè·³è¿‡æ­»é“¾ """
    if not url or url == "#": return False
    if "futuretools.io" in url: return True # å›½å¤–æºä½“æ£€å®¹æ˜“è¯¯æŠ¥ï¼Œç›´æ¥æ”¾è¡Œ
    try:
        res = requests.get(url, headers=HEADERS, timeout=8, stream=True)
        return res.status_code < 400
    except: return False

def clean_text(text):
    return ''.join(c for c in str(text) if ord(c) >= 32).strip() if text else ""

def classify_tool(desc, title):
    """ åˆ†ç±»é€»è¾‘ï¼šç¡®ä¿å›½å¤–å·¥å…·ä¹Ÿèƒ½åˆ†å…¥ efficiency """
    text = (title + " " + desc).lower()
    cost_score = sum(2 if kw in text else 0 for kw in COST_KEYWORDS)
    return "cost" if cost_score > 0 else "efficiency"

# --- å·¥å…·æŠ“å–ï¼šå›½å†…+å›½å¤– ---
def fetch_aibot():
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        cards = BeautifulSoup(res.text, 'html.parser').select('.url-card')[:20]
        for c in cards:
            tools.append({
                "title": c.select_one('strong').get_text(strip=True),
                "desc": c.select_one('.url-info p').get_text(strip=True),
                "source": c.select_one('a')['href']
            })
    except: pass
    return tools

def fetch_futuretools():
    print("ğŸŒ æ­£åœ¨çŒæ•ï¼šFutureTools (å…¨çƒæº)...")
    tools = []
    try:
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('div[role="article"]')[:20]
        for card in cards:
            t_elem = card.select_one('h2 a')
            if t_elem:
                tools.append({
                    "title": "ğŸŒ " + t_elem.get_text(strip=True),
                    "desc": "Global AI Tool Insight",
                    "source": t_elem['href']
                })
    except: pass
    return tools

# --- è¶‹åŠ¿é›·è¾¾ï¼šå¤šæºæ•´åˆ (36Kr + ITæ¡”å­ + æœºå™¨ä¹‹å¿ƒ) ---
def fetch_global_trends():
    print("ğŸ“¡ è¶‹åŠ¿é›·è¾¾ï¼šæ­£åœ¨æ‰«æå…¨ç½‘æƒ…æŠ¥...")
    trends = []
    
    # æº 1: 36Kr (ç»¼åˆå¿«è®¯)
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        items = BeautifulSoup(res.text, 'html.parser').select('a.article-title')[:3]
        for i in items:
            trends.append({"title": "ğŸ”¥ " + clean_text(i.get_text()), "desc": "36Kr å®æ—¶å¿«è®¯", "source": "https://36kr.com" + i['href']})
    except: pass

    # æº 2: æœºå™¨ä¹‹å¿ƒ (æ·±åº¦æŠ€æœ¯)
    try:
        res = requests.get("https://www.jiqizhixin.com/", headers=HEADERS, timeout=10)
        items = BeautifulSoup(res.text, 'html.parser').select('.article-title')[:2]
        for i in items:
            trends.append({"title": "ğŸ§  " + i.get_text(strip=True), "desc": "æœºå™¨ä¹‹å¿ƒæŠ€æœ¯æ·±åº¦", "source": "https://www.jiqizhixin.com" + i['href']})
    except: pass

    # æº 3: V2EX (ç‹¬ç«‹å¼€å‘/ç¤¾åŒºçƒ­ç‚¹)
    try:
        res = requests.get("https://www.v2ex.com/go/ai", headers=HEADERS, timeout=10)
        items = BeautifulSoup(res.text, 'html.parser').select('.item_title a')[:2]
        for i in items:
            trends.append({"title": "ğŸ’» " + i.get_text(strip=True), "desc": "V2EX ç¤¾åŒºçƒ­è®®", "source": "https://www.v2ex.com" + i['href']})
    except: pass

    # å…œåº•ï¼šå¦‚æœä¸Šé¢éƒ½æŒ‚äº†ï¼Œä¸è®©é¡µé¢ç©ºç™½
    if len(trends) < 3:
        trends.append({"title": "ğŸ’¡ AI åˆ›ä¸šè€…éœ€å…³æ³¨ï¼šæ¨¡å‹é™æœ¬ä¸ Agent è½åœ°", "desc": "è¡Œä¸šåˆ†æ", "source": "https://36kr.com"})
    
    return trends[:8] # æœ€å¤šæ˜¾ç¤º 8 æ¡

def main():
    print("ğŸš€ AI Hunter å…¨çƒå•†æœºç³»ç»Ÿå¯åŠ¨...")
    data = {"cost": [], "efficiency": [], "trend": []}
    
    # 1. æŠ“å–å·¥å…·
    raw_tools = fetch_aibot() + fetch_futuretools()
    
    # 2. å»é‡ä¸åˆ†ç±»
    seen = set()
    for t in raw_tools:
        name = t['title'].lower().strip()
        if name not in seen:
            seen.add(name)
            data[classify_tool(t['desc'], t['title'])].append(t)

    # 3. æŠ“å–å¤šæºè¶‹åŠ¿
    data["trend"] = fetch_global_trends()

    # 4. å†™å…¥æ–‡ä»¶
    try:
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ç«£å·¥ï¼æˆåŠŸå†™å…¥ {os.path.getsize('data.json')} å­—èŠ‚")
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
