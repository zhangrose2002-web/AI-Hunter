# -*- coding: utf-8 -*-
"""
AI Hunter - åˆ›ä¸šè€…åŠ å¼ºç‰ˆ (å…¨çƒçŒæ•)
ç›®æ ‡ï¼šä¸ºåˆ›ä¸šè€…ç²¾é€‰ é™æœ¬ã€å¢æ•ˆã€çœ‹è¶‹åŠ¿ çš„æ ¸å¿ƒå·¥å…·
"""

import json
import requests
from bs4 import BeautifulSoup
import sys
import time

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- å…³é”®è¯åº“ï¼šæ·±åº¦é€‚é…åˆ›ä¸šåœºæ™¯ ---
COST_KEYWORDS = [
    'å…è´¹', 'å¼€æº', 'é™æœ¬', 'æ›¿ä»£', 'è‡ªåŠ¨åŒ–', 'äººåŠ›', 'èŠ‚çœ', 'å®¢æœ', 'å¤–åŒ…', 'å¹³æ›¿',
    'free', 'open source', 'save cost', 'replace', 'automate', 'outsourcing', 'low code'
]

EFFICIENCY_KEYWORDS = [
    'æ•ˆç‡', 'ææ•ˆ', 'ä¸€é”®', 'ç”Ÿæˆ', 'æ‰¹é‡', 'æ™ºèƒ½', 'åŠå…¬', 'è¥é”€', 'å‰ªè¾‘', 'å†™ä½œ', 'PPT',
    'efficiency', 'productivity', 'boost', 'workflow', 'marketing', 'content creation'
]

TREND_KEYWORDS = [
    'çªç ´', 'å‘å¸ƒ', 'èèµ„', 'è¶‹åŠ¿', 'æŠ¥å‘Š', 'é¦–å‘', 'é‡ç£…', 'OpenAI', 'Sora', 'Claude',
    'breakthrough', 'funding', 'trend', 'report', 'unveiled', 'investment'
]

def clean_text(text):
    return ''.join(c for c in str(text) if ord(c) >= 32).strip() if text else ""

def classify_tool(desc, title):
    text = (title + " " + desc).lower()
    cost_score = sum(2 if kw in text else 0 for kw in COST_KEYWORDS)
    eff_score = sum(1 if kw in text else 0 for kw in EFFICIENCY_KEYWORDS)
    # åˆ›ä¸šè€…æ›´çœ‹é‡é™æœ¬ï¼Œæƒé‡ç¨é«˜
    return "cost" if cost_score >= eff_score and cost_score > 0 else "efficiency"

# ========================
# æ•çŒæº 1ï¼šAIå·¥å…·é›† (å›½å†…ä¼˜è´¨æº)
# ========================
def fetch_aibot(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šAIå·¥å…·é›† (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://ai-bot.cn/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('.url-card')[:max_items]
        for card in cards:
            title = card.select_one('strong').get_text(strip=True)
            desc = card.select_one('.url-info p').get_text(strip=True)
            link = card.select_one('a')['href']
            tools.append({"title": title, "desc": desc, "source": link})
    except Exception as e: print(f"âš ï¸ AIå·¥å…·é›†æ•è·è·³è¿‡: {e}")
    return tools

# ========================
# æ•çŒæº 2ï¼šå‘ç°AI (å›½å†…ä¼˜è´¨æº)
# ========================
def fetch_faxianai(max_items=15):
    print("ğŸ” çŒæ•ä¸­ï¼šå‘ç°AI (å›½å†…)...")
    tools = []
    try:
        res = requests.get("https://faxianai.com", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('a[href^="/tool/"]')[:max_items]
        for card in cards:
            title = card.select_one('h3').get_text(strip=True)
            desc = card.select_one('p').get_text(strip=True)
            source = "https://faxianai.com" + card['href']
            tools.append({"title": title, "desc": desc, "source": source})
    except Exception as e: print(f"âš ï¸ å‘ç°AIæ•è·è·³è¿‡: {e}")
    return tools

# ========================
# æ•çŒæº 3ï¼šFutureTools (å…¨çƒè§†é‡)
# ========================
def fetch_futuretools(max_items=20):
    print("ğŸ” çŒæ•ä¸­ï¼šFutureTools (å…¨çƒ)...")
    tools = []
    try:
        # æŠ“å–æŒ‰æ—¥æœŸæ’åºçš„æœ€æ–°å·¥å…·
        res = requests.get("https://www.futuretools.io/?sort=date-added", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('div[role="article"]')[:max_items]
        for card in cards:
            title_elem = card.select_one('h2 a')
            desc_elem = card.select_one('p')
            if title_elem:
                tools.append({
                    "title": title_elem.get_text(strip=True),
                    "desc": desc_elem.get_text(strip=True) if desc_elem else "",
                    "source": title_elem['href'] if title_elem.has_attr('href') else "#"
                })
    except Exception as e: print(f"âš ï¸ FutureToolsæ•è·è·³è¿‡: {e}")
    return tools

# ========================
# è¶‹åŠ¿æºï¼š36Kr AI ä¸“æ 
# ========================
def fetch_36kr_trends(max_items=5):
    print("ğŸ“¡ ç›‘æµ‹ä¸­ï¼š36Kr è¶‹åŠ¿é›·è¾¾...")
    trends = []
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        for item in soup.select('div.newsflash-item')[:15]: # æ‰©å¤§ç­›é€‰èŒƒå›´
            title_elem = item.select_one('a.article-title')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            # ä»…ç­›é€‰ä¸åˆ›ä¸š/AI å¼ºç›¸å…³çš„è¶‹åŠ¿
            if any(kw in title.lower() for kw in TREND_KEYWORDS + ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨äºº']):
                trends.append({
                    "title": title,
                    "desc": "åˆ›ä¸šè¶‹åŠ¿å¿«æŠ¥",
                    "source": "https://36kr.com" + title_elem['href']
                })
            if len(trends) >= max_items: break
    except Exception as e: print(f"âš ï¸ è¶‹åŠ¿æ•è·å¤±è´¥: {e}")
    return trends

def main():
    print("ğŸš€ AI Hunter å¯åŠ¨ï¼Œæ­£åœ¨ä¸ºåˆ›ä¸šè€…çŒæ•å…¨çƒå•†æœº...")
    
    # æ±‡æ€»æ‰€æœ‰å·¥å…·
    raw_tools = []
    raw_tools.extend(fetch_aibot(25))
    raw_tools.extend(fetch_faxianai(20))
    raw_tools.extend(fetch_futuretools(25))

    # å»é‡å¤„ç†
    unique_tools = []
    seen_titles = set()
    for t in raw_tools:
        name = t['title'].lower().strip()
        if name not in seen_titles:
            seen_titles.add(name)
            unique_tools.append(t)

    # åˆ†ç±»é€»è¾‘
    data = {"cost": [], "efficiency": [], "trend": []}
    for t in unique_tools:
        cat = classify_tool(t['desc'], t['title'])
        data[cat].append(t)

    # çŒæ•è¶‹åŠ¿
    data["trend"] = fetch_36kr_trends(6)

    # å…œåº•ï¼šå¦‚æœæŸé¡¹å¤ªå°‘ï¼Œä¿æŒä¹‹å‰çš„å±•ç¤º
    if len(data["cost"]) < 3:
        data["cost"].append({"title": "Claude 3.5 Sonnet", "desc": "é«˜æ€§ä»·æ¯”çš„æ™ºèƒ½æ¨¡å‹ï¼Œæ›¿ä»£åˆçº§åˆ†æå¸ˆ", "source": "https://claude.ai"})
    
    # å†™å…¥ JSON
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… çŒæ•å®Œæˆï¼")
    print(f"ğŸ’° å‘ç° {len(data['cost'])} ä¸ªé™æœ¬å·¥å…·")
    print(f"âš¡ å‘ç° {len(data['efficiency'])} ä¸ªå¢æ•ˆå·¥å…·")
    print(f"ğŸ“¡ æ•è· {len(data['trend'])} æ¡è¡Œä¸šè¶‹åŠ¿")

if __name__ == "__main__":
    main()
