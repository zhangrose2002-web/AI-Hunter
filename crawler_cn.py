# -*- coding: utf-8 -*-
"""
AI Hunter - å…¨çƒç‰ˆ
æŠ“å–å›½å†…å¤–çƒ­é—¨ AI å·¥å…·ï¼Œå¹¶æ™ºèƒ½åˆ†ç±»åˆ° cost / efficiency
"""

import json
import requests
from bs4 import BeautifulSoup
import time
import random

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# ========================
# åˆ†ç±»å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
# ========================

COST_KEYWORDS = [
    # ä¸­æ–‡
    'å®¢æœ', 'äººåŠ›', 'èŠ‚çœ', 'é™æœ¬', 'æ›¿ä»£', 'è‡ªåŠ¨åŒ–', 'å¤–åŒ…', 'å‡å°‘', 'ä½æˆæœ¬',
    'å…è´¹', 'å¼€æº', 'è®¡è´¹', 'é¢„ç®—', 'è´¢åŠ¡', 'æŠ¥é”€', 'åˆåŒ', 'æ³•åŠ¡', 'æ‹›è˜',
    # è‹±æ–‡
    'cost', 'save money', 'reduce cost', 'replace', 'automate', 'free', 'open source',
    'budget', 'cheaper', 'cut expenses', 'customer service', 'outsourcing'
]

EFFICIENCY_KEYWORDS = [
    # ä¸­æ–‡
    'æ•ˆç‡', 'æå‡', 'åŠ é€Ÿ', 'å¿«é€Ÿ', 'ä¸€é”®', 'è‡ªåŠ¨ç”Ÿæˆ', 'æ™ºèƒ½', 'ç§’å‡º', 'æ‰¹é‡',
    'è®¾è®¡', 'å‰ªè¾‘', 'å†™ä½œ', 'PPT', 'å‘¨æŠ¥', 'ä¼šè®®', 'ç¿»è¯‘', 'æŠ å›¾', 'æ’ç‰ˆ', 'ç»˜å›¾',
    # è‹±æ–‡
    'efficiency', 'boost', 'speed up', 'automate', 'generate', 'design', 'write',
    'edit', 'translate', 'create', 'productivity', 'workflow', 'fast', 'instant',
    'batch', 'summarize', 'analyze'
]

def classify_tool(desc, title):
    text = (str(title) + " " + str(desc)).lower()
    cost_score = sum(1 for kw in COST_KEYWORDS if kw in text)
    eff_score = sum(1 for kw in EFFICIENCY_KEYWORDS if kw in text)
    return "cost" if cost_score > eff_score else "efficiency"

def deduplicate(tools):
    """æ ¹æ®æ ‡é¢˜å»é‡"""
    seen = set()
    unique = []
    for t in tools:
        key = t['title'].strip().lower()
        if key not in seen:
            seen.add(key)
            unique.append(t)
    return unique

# ========================
# æŠ“å–å›½å†…ï¼šå‘ç°AI
# ========================

def fetch_faxianai(max_items=8):
    tools = []
    try:
        res = requests.get("https://faxianai.com", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        cards = soup.select('a[href^="/tool/"]')[:max_items]
        for card in cards:
            title = card.select_one('h3').get_text(strip=True) if card.select_one('h3') else ""
            desc = card.select_one('p').get_text(strip=True) if card.select_one('p') else ""
            tag = card.select_one('span.bg-blue-100').get_text(strip=True) if card.select_one('span.bg-blue-100') else ""
            source = "https://faxianai.com" + card['href']
            tools.append({"title": title, "desc": f"{desc} {tag}", "source": source})
    except Exception as e:
        print(f"âš ï¸ å‘ç°AIæŠ“å–å¤±è´¥: {e}")
    return tools

# ========================
# æŠ“å–å›½å¤–ï¼šFutureTools.ioï¼ˆæ¨èï¼ç»“æ„ç®€å•ï¼‰
# ========================

def fetch_futuretools(max_items=10):
    tools = []
    try:
        res = requests.get("https://www.futuretools.io", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        # FutureTools çš„å·¥å…·å¡ç‰‡
        cards = soup.select('div[role="article"]')[:max_items]
        for card in cards:
            title_elem = card.select_one('h2 a')
            desc_elem = card.select_one('p')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            desc = desc_elem.get_text(strip=True) if desc_elem else ""
            source = title_elem['href'] if title_elem.has_attr('href') else "#"
            tools.append({"title": title, "desc": desc, "source": source})
    except Exception as e:
        print(f"âš ï¸ FutureToolsæŠ“å–å¤±è´¥: {e}")
    return tools

# ========================
# æ‰‹åŠ¨å…œåº•ï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
# ========================

def get_manual_tools():
    return {
        "cost": [
            {"title": "Doubao (è±†åŒ…)", "desc": "Free AI assistant for customer service", "source": "https://www.doubao.com"},
            {"title": "WPS AI", "desc": "Automate office tasks, reduce software costs", "source": "https://www.wps.cn/ai"}
        ],
        "efficiency": [
            {"title": "Meitu (ç¾å›¾ç§€ç§€)", "desc": "AI photo editing in seconds", "source": "https://xiuxiu.meitu.com"},
            {"title": "Qwen (é€šä¹‰åƒé—®)", "desc": "Generate reports, emails, and summaries instantly", "source": "https://qwen.ai"},
            {"title": "Canva Magic Studio", "desc": "Create designs with text prompts", "source": "https://www.canva.com/magic-studio/"},
            {"title": "Notion AI", "desc": "Write, summarize, and organize your work", "source": "https://www.notion.so/product/ai"}
        ]
    }

# ========================
# è¶‹åŠ¿æ–°é—»ï¼ˆä¿ç•™ï¼‰
# ========================

def get_trend_news(max_items=3):
    try:
        res = requests.get("https://36kr.com/newsflashes", headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        items = []
        for item in soup.select('div.newsflash-item')[:max_items]:
            title_elem = item.select_one('a.article-title')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            link = "https://36kr.com" + title_elem['href'] if title_elem.has_attr('href') else "#"
            if any(kw in title for kw in ['AI', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'AIGC']):
                items.append({"title": title, "desc": "Source: 36Kr", "source": link})
        return items or [{"title": "Global AI adoption accelerates", "desc": "Enterprise demand surges", "source": "https://36kr.com"}]
    except:
        return [{"title": "Trends loading...", "desc": "Check back later", "source": "#"}]

# ========================
# ä¸»ç¨‹åº
# ========================

def main():
    print("ğŸŒ å¼€å§‹æŠ“å–å…¨çƒ AI å·¥å…·...")

    all_tools = []

    # æŠ“å–å›½å†…å¤–
    print("ğŸ‡¨ğŸ‡³ æŠ“å– å‘ç°AI...")
    all_tools.extend(fetch_faxianai())
    
    print("ğŸŒ æŠ“å– FutureTools...")
    all_tools.extend(fetch_futuretools())

    # å»é‡
    all_tools = deduplicate(all_tools)

    # åˆ†ç±»
    cost_list = []
    efficiency_list = []
    for tool in all_tools:
        category = classify_tool(tool['desc'], tool['title'])
        if category == "cost":
            cost_list.append(tool)
        else:
            efficiency_list.append(tool)

    # å¦‚æœæŠ“å–ç»“æœå¤ªå°‘ï¼Œè¡¥å……æ‰‹åŠ¨æ•°æ®
    manual = get_manual_tools()
    if len(cost_list) < 2:
        cost_list = manual["cost"]
    if len(efficiency_list) < 3:
        efficiency_list = manual["efficiency"]

    # è·å–è¶‹åŠ¿
    trend = get_trend_news()

    # ä¿å­˜
    data = {
        "cost": cost_list,
        "efficiency": efficiency_list,
        "trend": trend
    }

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… æˆåŠŸç”Ÿæˆ data.jsonï¼")
    print(f"   - æˆæœ¬æ€æ‰‹: {len(cost_list)} ä¸ªï¼ˆå«å›½é™…å·¥å…·ï¼‰")
    print(f"   - æ•ˆç‡å€å¢: {len(efficiency_list)} ä¸ªï¼ˆå«å›½é™…å·¥å…·ï¼‰")
    print(f"   - è¶‹åŠ¿é›·è¾¾: {len(trend)} æ¡")

if __name__ == "__main__":
    main()