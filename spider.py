import os
import json
import ftplib
import time
import random  # ç¡®ä¿è¿™ä¸ªåœ¨è¿™é‡Œ
import urllib.parse
import requests
from bs4 import BeautifulSoup
from datetime import datetime

def fetch_industry_leads():
    # 1. ç²¾ç®€å…³é”®è¯ï¼ˆå»æ‰æ‰€æœ‰å¼•å·å’ŒåŠ å·ï¼Œæé«˜æœç´¢æˆåŠŸç‡ï¼‰
  raw_keywords = [
        "åŠå¯¼ä½“ æ‹›æ ‡å…¬å‘Š", 
        "é›†æˆç”µè·¯ æ‰©äº§ æ–°é—»", 
        "å°æµ‹å‚ é‡‡è´­ å›ºæ™¶æœº", 
        "é€šå¯Œå¾®ç”µ å®˜æ–¹å…¬å‘Š", 
        "é•¿ç”µç§‘æŠ€ æ‰©äº§é¡¹ç›®",
        "åå¤©ç§‘æŠ€ æ‹›æ ‡",
        "åŠå¯¼ä½“ å°æµ‹ åŸºåœ° æŠ•äº§"
    ]
    
    import random
    selected_kws = random.sample(raw_keywords, min(5, len(raw_keywords)))
    real_leads = []
    
    # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé˜²æ­¢è¢«å°
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    print(f"ğŸ“¡ æ­£åœ¨æ·±åº¦æ‰«æä»¥ä¸‹é¢†åŸŸ: {selected_kws}")

    for kw in selected_kws:
        query = urllib.parse.quote(kw)
        # æ¢ç”¨å¿…åº”çš„å›½é™…ç‰ˆæ¥å£ï¼Œæœç´¢ç»“æœæ›´ç¨³å®š
        url = f"https://www.bing.com/search?q={query}&form=QBLH"
        
        try:
            time.sleep(2) # å¢åŠ å»¶è¿Ÿï¼Œé˜²æ­¢è¢«å°
            response = requests.get(url, headers=headers, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å…¼å®¹æ€§è§£æï¼šå°è¯•å¤šç§å¯èƒ½çš„æœç´¢ç»“æœæ ‡ç­¾
            items = soup.select('.b_algo') or soup.select('li.b_algo')
            
            for item in items[:10]:
                title = item.find('h2').get_text() if item.find('h2') else ""
                link = item.find('a')['href'] if item.find('a') else "#"
                snippet = item.find('p').get_text() if item.find('p') else "æŸ¥çœ‹è¯¦æƒ…..."
                
                if title:
                    real_leads.append({
                        "id": int(datetime.now().timestamp()) + random.randint(1, 9999),
                        "company": title[:30].strip(),
                        "location": "å…¨å›½/å®æ—¶",
                        "category": "domestic",
                        "tag": kw.split()[0], 
                        "reason": snippet[:120] + "...",
                        "website": link,
                        "phone": "ç™»å½•å®˜ç½‘æŸ¥è¯¢"
                    })
            print(f"âœ… å·²è·å– [{kw}] ç›¸å…³çº¿ç´¢")
        except Exception as e:
            print(f"âš ï¸ æ‰«æ [{kw}] å¤±è´¥: {e}")

    # ğŸ†˜ æ ¸å¿ƒè¡¥ä¸ï¼šå¦‚æœçœŸçš„ä»€ä¹ˆéƒ½æ²¡æœåˆ°ï¼Œå¼ºåˆ¶ç”Ÿæˆâ€œä¿åº•çº¿ç´¢â€ï¼Œä¸è®©é¡µé¢å˜ç™½
    if not real_leads:
        print("âš ï¸ å®æ—¶æŠ“å–ä¸ºç©ºï¼Œæ³¨å…¥è¡Œä¸šæ ‡æ†æ•°æ®...")
        real_leads = [
            {
                "id": 1,
                "company": "ç³»ç»Ÿæƒ…æŠ¥ï¼šå¼•æ“æ­£åœ¨è½®è¯¢ä¸­",
                "location": "å¾…æ›´æ–°",
                "category": "domestic",
                "tag": "ç³»ç»ŸçŠ¶æ€",
                "reason": "ç”±äºæœç´¢å¼•æ“é¢‘ç‡é™åˆ¶ï¼Œå®æ—¶çº¿ç´¢æ­£åœ¨æ’é˜ŸæŠ“å–ã€‚è¯·5åˆ†é’Ÿååˆ·æ–°ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨å‘ˆç°æœ€æ–°çš„å°æµ‹æ‹›æ ‡ä¿¡æ¯ã€‚",
                "website": "https://www.insight-ai.com",
                "phone": "ç›‘æ§ä¸­"
            }
        ]
    
    return real_leads

# ==========================================
# 2. ç”Ÿæˆ JSON æ•°æ®æ–‡ä»¶ (ä¿æŒä¸å˜)
# ==========================================
def save_to_json(data):
    file_path = 'data.json'
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®å·²æˆåŠŸå†™å…¥æœ¬åœ° {file_path}")
    except Exception as e:
        print(f"âŒ å†™å…¥ JSON å¤±è´¥: {e}")

# ==========================================
# 3. ä¼ å›é˜¿é‡Œäº‘è™šæ‹Ÿç©ºé—´ (ä¿æŒä¸å˜)
# ==========================================
def upload_to_server():
    FTP_SERVER = "qxu1590320302.my3w.com"
    FTP_USER = "qxu1590320302"
    FTP_PASS = "123456ab"

    try:
        print(f"æ­£åœ¨è¿æ¥ FTP: {FTP_SERVER} ...")
        session = ftplib.FTP()
        session.connect(FTP_SERVER, 21, timeout=30)
        session.login(FTP_USER, FTP_PASS)
        session.set_pasv(True)
        
        try:
            session.cwd('/htdocs')
        except:
            print("å·²ç»åœ¨æ ¹ç›®å½•æˆ– htdocs æ— æ³•è®¿é—®")
        
        # ç¡®ä¿åŒæ­¥æœ€æ–°çš„ä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶
        files_to_send = ['index.html', 'spider.html', 'data.json']
        
        for file_name in files_to_send:
            if os.path.exists(file_name):
                with open(file_name, 'rb') as f:
                    session.storbinary(f'STOR {file_name}', f)
                    print(f"ğŸš€ å·²æˆåŠŸåŒæ­¥åˆ°ç©ºé—´: {file_name}")
            else:
                print(f"âš ï¸ è·³è¿‡: æœ¬åœ°æœªæ‰¾åˆ° {file_name}")

        session.quit()
        print(f"âœ¨ å®æ—¶åŒæ­¥å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¼ è¾“å¤±è´¥: {e}")

# ==========================================
# 4. ç»Ÿä¸€æ‰§è¡Œå…¥å£
# ==========================================
if __name__ == "__main__":
    leads = fetch_industry_leads()
    save_to_json(leads)
    # æš‚æ—¶æ³¨é‡Šæ‰ FTPï¼Œå…ˆç¡®ä¿ GitHub è¿™è¾¹èƒ½è·‘é€š
    # upload_to_server()

