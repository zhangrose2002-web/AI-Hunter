import os
import json
import ftplib
import time
import random  # ç¡®ä¿è¿™ä¸ªåœ¨è¿™é‡Œ
import urllib.parse
import requests
from bs4 import BeautifulSoup
from datetime import datetime

def fetch_industry_leads():
    # --- æ ¸å¿ƒå…³é”®è¯åº“ ---
    # åŒ…å«å•è¯ç›‘æ§å’Œç»„åˆé€»è¾‘ç›‘æ§
    raw_keywords = [
        "800Gå…‰æ¨¡å—", "EMLæ¿€å…‰å™¨", "é«˜é€Ÿå…‰æ”¶å‘", "CPOæŠ€æœ¯", "äº§çº¿æ‰©èƒ½", 
        "è½¦è§„çº§è®¤è¯", "IGBTæ¨¡å—", "SiCåŠŸç‡å™¨ä»¶", "æ–°å¢äº§çº¿æ‹›æ ‡", "OBCå°è£…", 
        "çŸ³è‹±æ™¶ä½“æŒ¯è¡å™¨", "KDS/ç²¾å·¥æ›¿ä»£", "SMDå°è£…", "é¢‘ç‡å…ƒä»¶", "äº§èƒ½ç¿»å€", 
        "å¾®æ³¢ç»„ä»¶", "åšè†œç”µè·¯", "é‡‘å±ç®¡å£³å°è£…", "å›½äº§åŒ–æ›¿ä»£", "è‡ªä¸»å¯æ§", 
        "MEMSä¼ æ„Ÿå™¨", "çº¢å¤–æ¢æµ‹å™¨", "çœŸç©ºå°è£…", "å°æ‰¹é‡è¯•äº§", "å·¥è‰ºç ”å‘", 
        "å…ˆè¿›å°è£…", "æ°”å¯†æ€§æµ‹è¯•", "ç³»ç»Ÿçº§å°è£…(SiP)", "å…ˆè¿›å°æµ‹é¡¹ç›®å…¬ç¤º", 
        "TO-CANå°è£…", "æ¿€å…‰é›·è¾¾", "å…‰ç”µæ¢æµ‹å™¨", "äºŒæç®¡å°è£…", "æ‰©å»ºå‚æˆ¿",
        "äº§èƒ½ç¿»å€ TO-CANå°è£…", "äº§çº¿æ‰©èƒ½ IGBTæ¨¡å—å°è£…", 
        "å¢äº§ å…‰æ”¶å‘ç»„ä»¶(TOSA)", "è‡ªä¸»å¯æ§ æ°”å¯†æ€§å°è£…è®¾å¤‡", 
        "å›½äº§æ›¿ä»£ çœŸç©ºå¹³è¡Œç¼ç„Šæœº", "æ ¸å¿ƒè£…å¤‡ å¾®æ³¢ç»„ä»¶å°è£…", 
        "å°æ‰¹é‡è¯•äº§ SiCåŠŸç‡æ¨¡å—", "å·¥è‰ºç ”å‘ MEMSçœŸç©ºå°è£…", 
        "æ‰“æ · æ¿€å…‰å°ç„Šå·¥è‰º"
    ]

    print(f"ğŸš€ å¼•æ“å¯åŠ¨ï¼šæ­£åœ¨å¯¹ {len(raw_keywords)} ç»„æ ¸å¿ƒå…³é”®è¯è¿›è¡Œæ·±åº¦çº¿ç´¢æ¢æµ‹...")
    real_leads = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
    }

    # ä¸ºäº†é¿å…è¢«æœç´¢å¼•æ“å°ç¦ï¼Œæˆ‘ä»¬éšæœºæŠ½å– 15ç»„å…³é”®è¯è¿›è¡Œå•æ¬¡è½®è¯¢
    import random
    selected_kws = random.sample(raw_keywords, min(15, len(raw_keywords)))

    for kw in selected_kws:
        # å¤„ç†ç»„åˆæœç´¢é€»è¾‘ï¼šæŠŠ "A" + "B" è½¬æ¢ä¸ºæœç´¢å¼•æ“è¯†åˆ«çš„ A B
        search_query = kw.replace('"', '').replace('+', ' ')
        encoded_query = urllib.parse.quote(search_query)
        
        # ä½¿ç”¨ Bing æœç´¢è¿›è¡Œå…¨ç½‘æ¢æµ‹
        url = f"https://www.bing.com/search?q={encoded_query}"
        
        try:
            time.sleep(1) # é¿å¼€é¢‘ç‡é™åˆ¶
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è§£ææœç´¢ç»“æœ
            items = soup.find_all('li', class_='b_algo', limit=2) # æ¯ä¸ªè¯å–å‰2æ¡æœ€ç›¸å…³çš„
            for i, item in enumerate(items):
                title_elem = item.find('h2')
                snippet_elem = item.find('p')
                link_elem = item.find('a')

                if title_elem and link_elem:
                    real_leads.append({
                        "id": int(datetime.now().timestamp()) + random.randint(1, 1000),
                        "company": title_elem.text[:25], # æˆªå–æ ‡é¢˜å‰æ®µä½œä¸ºå‚è€ƒæœºæ„
                        "location": "å…¨ç½‘æ¢æµ‹",
                        "category": "domestic" if "æ›¿ä»£" in kw or "å›½äº§" in kw else "intl",
                        "tag": kw.replace('"', '').split('+')[0].strip(), # æå–ç¬¬ä¸€ä¸ªå…³é”®è¯åšæ ‡ç­¾
                        "reason": snippet_elem.text[:100] if snippet_elem else "ç‚¹å‡»é“¾æ¥æŸ¥çœ‹è¯¦ç»†æ‹›æ ‡/æ‰©äº§è¯¦æƒ…...",
                        "website": link_elem['href'],
                        "phone": "è§è¯¦æƒ…é¡µå…¬ç¤º"
                    })
            print(f"âœ… å…³é”®è¯ [{kw}] æ¢æµ‹å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å…³é”®è¯ [{kw}] æŠ“å–å¼‚å¸¸: {e}")

    if not real_leads:
        print("âš ï¸ æœ¬æ¬¡æœªæ¢æµ‹åˆ°å®æ—¶åŠ¨æ€ï¼Œå¯ç”¨è¡Œä¸šå¸¸æ€çº¿ç´¢...")
        real_leads = [
            {
                "id": 999,
                "company": "è¡Œä¸šåŠ¨æ€ç›‘æ§ä¸­",
                "location": "å…¨å›½",
                "category": "domestic",
                "tag": "ç³»ç»Ÿæç¤º",
                "reason": "å½“å‰å®æ—¶æœç´¢æœªå‘ç°æ–°å…¬å‘Šï¼Œæ­£åœ¨æ‰©å¤§èŒƒå›´ç›‘æ§ 45 ç»„æ ¸å¿ƒå…³é”®è¯...",
                "website": "#",
                "phone": "-"
            }
        ]

    return real_leads

# ==========================================
# 2. ç”Ÿæˆ JSON æ•°æ®æ–‡ä»¶ (ä¿æŒä¸å˜)
# ==========================================
def save_to_json(data):
    file_path = 'data.json'
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®å·²æˆåŠŸå†™å…¥æœ¬åœ° {file_path}")
    except Exception as e:
        print(f"âŒ å†™å…¥ JSON å¤±è´¥: {e}")

# ==========================================
# 3. ä¼ å›é˜¿é‡Œäº‘è™šæ‹Ÿç©ºé—´ (ä¿æŒä¸å˜)
# ==========================================
def upload_to_server():
    FTP_SERVER = "qxu1590320302.my3w.com"
    FTP_USER = "qxu1590320302"
    FTP_PASS = "123456ab"

    try:
        print(f"æ­£åœ¨è¿æ¥ FTP: {FTP_SERVER} ...")
        session = ftplib.FTP()
        session.connect(FTP_SERVER, 21, timeout=30)
        session.login(FTP_USER, FTP_PASS)
        session.set_pasv(True)
        
        try:
            session.cwd('/htdocs')
        except:
            print("å·²ç»åœ¨æ ¹ç›®å½•æˆ– htdocs æ— æ³•è®¿é—®")
        
        # ç¡®ä¿åŒæ­¥æœ€æ–°çš„ä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶
        files_to_send = ['index.html', 'spider.html', 'data.json']
        
        for file_name in files_to_send:
            if os.path.exists(file_name):
                with open(file_name, 'rb') as f:
                    session.storbinary(f'STOR {file_name}', f)
                    print(f"ğŸš€ å·²æˆåŠŸåŒæ­¥åˆ°ç©ºé—´: {file_name}")
            else:
                print(f"âš ï¸ è·³è¿‡: æœ¬åœ°æœªæ‰¾åˆ° {file_name}")

        session.quit()
        print(f"âœ¨ å®æ—¶åŒæ­¥å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¼ è¾“å¤±è´¥: {e}")

# ==========================================
# 4. ç»Ÿä¸€æ‰§è¡Œå…¥å£
# ==========================================
if __name__ == "__main__":
    leads = fetch_industry_leads()
    save_to_json(leads)
    upload_to_server()





